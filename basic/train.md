# train过程中的问题

## 梯度弥散和梯度爆炸

参考：https://cloud.tencent.com/developer/article/1052770

### 梯度弥散

#### 定义

梯度在反传过程中，幅度越来越小，导致反传回浅层的梯度特别小，导致浅层网络更新缓慢，学习不到有价值的特征。梯度弥散就是梯度消失。

#### 产生原因

##### 激活函数角度

见op.md中的激活函数sigmoid

##### 深层网络角度

设网络中某一全连接层（卷积层也一样）权重为$W$，输入改成的数据为$X$，该层的输出为$Y$，

该层的前传为
$$
Y=XW
$$
反传为
$$
dX=dY\ W^T
$$
可以发现，如果W的值过小，远小于1，那么dX的值就会特别小，dX再往前一层反传，如果前一层的W也特别小，那么前一层输入的梯度再相乘之后就会更小，如此继续反传，在深层的网络中梯度就会逐渐接近于0，即发生梯度弥散。

相反的，如果W的值过大，远大于1，那么随着反传，梯度就会越来越大，即发生梯度爆炸。

#### 解决方法

1. 针对激活函数问题，用relu代替sigmoid
2. 针对深层网络问题
   1. 加入batchnorm，work的原因见batchnorm.md里的优点一 防止梯度弥散，加速训练收敛中的权重伸缩不变性角度
   2. 采用ResNet结构，跨层连接结构可以使梯度反传时直接通过shortcut传递到上一层，最后一层的梯度都可以通过shortcut传入第一层。ResNet可以说一定程度上完全解决了梯度弥散问题，可以将网络深度加深到特别深。

### 梯度爆炸

参考：https://www.jiqizhixin.com/articles/2017-12-21-14

#### 定义

梯度在反传过程中，幅度越来越大，导致网络权重的大幅更新，导致训练过程不稳定，极端情况下权重会发生溢出，变成NaN。

#### 产生原因

即深层网络角度中的W值过大的情况，一言以蔽之，网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。

一般来说梯度爆炸很少遇到，更多的是梯度弥散。

#### 解决方法

1. 加入batchnorm，work的原因见batchnorm.md里的优点一 防止梯度弥散，加速训练收敛中的权重伸缩不变性角度
2. 梯度截断，如果梯度值过大超过设定的阈值，就进行截断，强制梯度在阈值范围内
3. 正则化，L2Norm，如果发生了梯度爆炸，就会使权值的二范数特别大，从而使loss变得特别大，loss要减小就要抑制梯度爆炸的发生。

## 不收敛

参考：https://blog.ailemon.me/2019/02/26/solution-to-loss-doesnt-drop-in-nn-train/

## 过拟合

参考：https://blog.csdn.net/CV_YOU/article/details/89416210

从数据上，从模型上

过拟合其实就是过分学习了某一些特征，过分学习了训练集在这一些特征上所体现出来的特性，而这些特性并不是网络表征的任务所需要的。

### 从数据上

1. **数据增强**

   数据增强就是扩大样本量，使得原本训练集在一些非关键特征上体现出的共性因数据增强而消失。

2. **加噪**

   加噪的作用和数据增强一样，就是使得训练集在一些非关键特征上体现出的共性因加噪而消失。

### 从模型上

1. L2Norm（weight decay）
2. L1Norm
3. batchnorm
4. dropout
5. model ensemble boosting bagging
6. simpler model

### 从训练技巧上

1. early stopping 

## 学习率的选择

