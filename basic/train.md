# train过程中的问题

## 梯度弥散和梯度爆炸

参考：https://cloud.tencent.com/developer/article/1052770

### 梯度弥散

#### 定义

梯度在反传过程中，幅度越来越小，导致反传回浅层的梯度特别小，导致浅层网络更新缓慢，学习不到有价值的特征。梯度弥散就是梯度消失。

#### 产生原因

##### 激活函数角度

见op.md中的激活函数sigmoid

##### 深层网络角度

设网络中某一全连接层（卷积层也一样）权重为$W$，输入改成的数据为$X$，该层的输出为$Y$，

该层的前传为
$$
Y=XW
$$
反传为
$$
dX=dY\ W^T
$$
可以发现，如果W的值过小，远小于1，那么dX的值就会特别小，dX再往前一层反传，如果前一层的W也特别小，那么前一层输入的梯度再相乘之后就会更小，如此继续反传，在深层的网络中梯度就会逐渐接近于0，即发生梯度弥散。

相反的，如果W的值过大，远大于1，那么随着反传，梯度就会越来越大，即发生梯度爆炸。

#### 解决方法

1. 针对激活函数问题，用relu代替sigmoid
2. 针对深层网络问题
   1. 加入batchnorm，work的原因见batchnorm.md里的优点一 防止梯度弥散，加速训练收敛中的权重伸缩不变性角度
   2. 采用ResNet结构，跨层连接结构可以使梯度反传时直接通过shortcut传递到上一层，最后一层的梯度都可以通过shortcut传入第一层。ResNet可以说一定程度上完全解决了梯度弥散问题，可以将网络深度加深到特别深。

### 梯度爆炸

参考：https://www.jiqizhixin.com/articles/2017-12-21-14

#### 定义

梯度在反传过程中，幅度越来越大，导致网络权重的大幅更新，导致训练过程不稳定，极端情况下权重会发生溢出，变成NaN。

#### 产生原因

即深层网络角度中的W值过大的情况，一言以蔽之，网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。

一般来说梯度爆炸很少遇到，更多的是梯度弥散。

#### 解决方法

1. 加入batchnorm，work的原因见batchnorm.md里的优点一 防止梯度弥散，加速训练收敛中的权重伸缩不变性角度
2. 梯度截断，如果梯度值过大超过设定的阈值，就进行截断，强制梯度在阈值范围内
3. 正则化，L2Norm，如果发生了梯度爆炸，就会使权值的二范数特别大，从而使loss变得特别大，loss要减小就要抑制梯度爆炸的发生。

## 不收敛（欠拟合）

参考：https://blog.ailemon.me/2019/02/26/solution-to-loss-doesnt-drop-in-nn-train/

### 不收敛的原因和对应的解决方法

#### 从数据上

1. 数据集本身有问题

   - 存在大量标注错误和噪声
   - 数据集没有训练价值，是白噪声

2. feature enginering没有做好，提取的数据特征不是关键特征

3. 没有做feature归一化

4. 没有shuffle数据集（这个既可以说是过拟合，也可以说是欠拟合，看不同的角度）

   不做shuffle，每次迭代都会使用一批相同label的数据，导致网络会偏向优化这个label所对应的参数（例如全连接分类层，只去优化这一label对应的权重向量），其他参数没有得到很好的学习，再优化下一批数据时，换了label，loss就会回到特别大的值，这样一直震荡，就很难收敛。

#### 从模型上

1. 网络的拟合能力capacity不足

   用小量数据来训练，看是否有loss下降甚至出现过拟合，如果小量数据可以收敛甚至过拟合，就说明是网络拟合能力不足，只能拟合小部分数据，无法拟合大量数据。

   解决方法：换更大的网络，增加网络层数。

2. 参数初始化有问题

   一般用xavier

3. 发生了梯度弥散或梯度爆炸导致训练停滞难以继续

4. 正则化设置过度

   网络会过于去优化正则项，而对loss本身没有很好优化

5. 损失函数选择不恰当

#### 从训练上

1. 优化器选择不恰当

2. 学习率问题

   学习率到后期还保持比较大的话，会导致loss产生震荡，即不下降，很难继续收敛。

   解决方法：做lr schedule，到后期逐渐减小lr

## 过拟合

参考：

https://blog.csdn.net/CV_YOU/article/details/89416210

http://wangwlj.com/2017/10/18/DL_overfitting/

**过拟合其实就是过分学习了某一些特征，过分学习了训练集在这一些特征上所体现出来的特性，而这些特性并不是网络表征的任务所需要的。**

**减小过拟合就是使网络学习到更鲁棒更本质的特征。**

从数据上，从模型上，从训练技巧上三个方面的解决措施：

### 从数据上

#### 1. 数据增强

数据增强就是扩大样本量，使得原本训练集在一些非关键特征上体现出的共性因数据增强而消失。

#### 2. 数据加噪

加噪的作用和数据增强一样，就是使得训练集在一些非关键特征上体现出的共性因加噪而消失。

### 从模型上

#### 1. L2正则化（weight decay）

参考：https://blog.csdn.net/jinping_shi/article/details/52433975
$$
loss = loss\ function(w;x)+\lambda||w||_2^2
$$
其中，$\lambda$为正则化系数，越大，表明正则化程度越强，对过拟合抑制力度就越大，但过大会导致模型不收敛。

$||w||_2$表示参数（权重）向量的二范数，为所有参数的平方和再求平方根，e.g.有n个参数$w_1,w_2,...,w_n$，那么$||w||_2=\sqrt{w_1^2+w_2^2+...+w_n^2}$，$||w||_2^2=w_1^2+w_2^2+...+w_n^2$

##### L2Norm能抑制过拟合的原因

将参数向量的二范数加入loss，在训练中，最小化loss会使每个参数的绝对值都尽可能的小，最终**构建一个所有参数都比较小的模型，从而抑制过拟合**。关于参数小可以抑制过拟合，有两种解释：

1. 如果模型参数很大，那么输入数据只要有一点偏移，就会输出产生很大的偏移，尤其是在非关键特征上的偏移，也会导致输出的大偏移，这显然就是过拟合了。而如果**模型参数足够小，那么即使输入数据偏的多一点，也不会对输出有很大的影响，可以说是抗扰动能力强**。
2. 所有模型参数都足够小，会使网络不会向某一维特征过分偏移，也就减轻了过拟合。

#### 2. L1正则化

**通过产生稀疏的参数矩阵，来特征选择，从而抑制过拟合。**

参考：

https://blog.csdn.net/jinping_shi/article/details/52433975

https://www.jianshu.com/p/8de32549d386
$$
loss = loss\ function(w;x)+\lambda||w||_1
$$
$||w||_1$表示参数（权重）向量的一范数，为所有参数的绝对值的和，e.g.有n个参数$w_1,w_2,...,w_n$，那么$||w||_1=|w_1|+|w_2|+...+|w_n|$

可以发现，**加入L1正则化后，损失函数变成不可导的了**，因为绝对值函数是不可导的。不可导就无法使用梯度下降法来优化，那么怎么解决呢？方法是：**proximal operator**

##### proximal operator

![img](https://upload-images.jianshu.io/upload_images/5756726-0262b334b06676fe.png?imageMogr2/auto-orient/strip|imageView2/2/w/644/format/webp)

自己整理一下，加入L1正则化后目标函数为
$$
\min_w(lf(w)+\lambda||w||_1)
$$
迭代更新方法为
$$
w_{t}=Prox_{h,\alpha}(w_{t-1}-\alpha\nabla w_t)
$$
其中，$\alpha$为学习率，Prox函数为
$$
Prox_{h,\alpha}(x)=\arg \min_y(\frac{1}{2\alpha}||y-x||^2+\lambda||y||_1)
$$
需要注意，x和y都是向量，且y和x的维度是一致的，以x为一维为例，
$$
Prox_{h,\alpha}(x)=\arg \min_{y}(\frac{1}{2\alpha}(y-x)^2+\lambda|y|)
$$
记
$$
g(y)=\frac{1}{2\alpha}(y-x)^2+\lambda|y|
$$
写成分段函数形式
$$
g(y)=\begin{cases}\frac{1}{2\alpha}(y-x)^2+\lambda y & y>0\\
\frac{1}{2\alpha}(y-x)^2-\lambda y & y<0\end{cases}
$$
$y>0$时，
$$
0=\frac{\part g}{\part y}=\frac{1}{\alpha}(y-x)+\lambda
$$
得到，$y=x-\alpha$，此时条件是$y>0$，所以需要$x-\alpha>0$才能取到最小值，若不满足，则$y=0$时取最小值。

$y<0$时，
$$
0=\frac{\part g}{\part y}=\frac{1}{\alpha}(y-x)-\lambda
$$
得到，$y=x+\alpha$，此时条件是$y<0$，所以需要$x+\alpha<0$才能取到最小值，若不满足，则$y=0$时取最小值。

所以，综上，并推广到n维向量情况，有
$$
Prox_{h,\alpha}(x_i)=\begin{cases}x_i-\lambda\alpha & if \ x_i-\lambda\alpha>0 \\
x_i+\lambda\alpha & if \ x_i+\lambda\alpha<0\\
0 & other\end{cases}
$$
可以发现，**加入L1正则化后的损失函数，在proximal operator参数更新策略中，很多参数不满足上述分段函数中前两种条件，这些参数就会变为0**，所以**L1正则化会使模型参数矩阵成为稀疏矩阵**，即参数中只有少量非零元素，从而实现了**特征选择**，**保留重要特征，忽略不重要（非关键）特征**，进而可以抑制过拟合。

正则化系数$\lambda$的影响：$\lambda$越大，满足分段函数上两个条件的参数就越少，就有越多的参数为0，参数矩阵就更稀疏，抑制过拟合的力度就越大。反之，$\lambda$越小，极端情况下为0，那么所有参数都可以满足上两个条件，就不会产生稀疏矩阵，就没有特征选择和抑制过拟合的作用。

#### 补充：L1和L2正则化引入的先验

参考：https://zhuanlan.zhihu.com/p/116079663

**不加正则化项的机器学习模型其实就是极大似然估计的过程，因为极大似然估计就是寻找使得模型最符合当前数据（训练集）的参数，而机器学习模型也是通过训练使得模型最符合训练集，因为训练的目标就是降低训练集的loss。**

设输入为$X$，输出为$y$，模型参数为$w$，那么对数似然函数为
$$
\log P(y|X,w)
$$
但在概率统计中，还有一种参数估计方法，是极大后验估计。**相比于极大似然估计，极大后验估计给参数加了一个先验，防止其过于注重训练集的数据，从而过拟合，从这一点上说，极大后验估计和正则化产生了逻辑相通。所以，加入正则化项就相当于给参数引入了先验。**

从公式上证明这一点，并求出L1和L2各自给参数引入了什么先验。

首先，后验概率为
$$
\log P(y|X,w)P(w)=\log P(y|X,w)+\log P(w)
$$
若假设参数$w$服从高斯分布，即$w\sim N(0,\sigma^2)$，那么
$$
\log P(w)=\log \prod_{j}P(w_j)=\log \prod_{j}\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{w_j^2}{2\sigma^2})=-\frac{1}{2\sigma^2}\sum_{j}w_j^2+C
$$
可以发现，$\log P(w)$就是等价于在损失函数中增加了L2正则化项，所以**L2正则化引入的先验就是参数服从高斯分布。**

若参数$w$服从参数为a的拉普拉斯分布，即$P(w_j)=\frac{1}{\sqrt{2a}}\exp(-\frac{|w_j|}{a})$，那么
$$
\log P(w)=\log \prod_{j}P(w_j)=\log \prod_{j}\frac{1}{\sqrt{2a}}\exp(-\frac{|w_j|}{a})=-\frac{1}{a}\sum_j|w_j|+C
$$
可以发现，$\log P(w)$就是等价于在损失函数中增加了L1正则化项，所以**L1正则化引入的先验就是参数服从拉普拉斯分布。**

注：极大化后验概率相当于极小化其相反数，所以对后验概率的表达式加上负号，就变成了损失函数，而上面求出的$\log P(w)$都是带有负号的，所以整体加负号后，这一项是正号加上去，这与一般模型损失函数加正则化的方式是一致的。

#### 3. batchnorm

work的原因：见batchnorm.md batchnorm的优点二

1. 从样本角度（数据角度）

   训练过程中，BN层会使一个batch内的所有样本关联在一起，即每个样本的输出都受到batch内所有样本的影响，相当于间接做了数据增强，达到减轻过拟合的作用。

2. 从特征角度（模型角度）

   BN层使输出的特征每个维度都服从类似的分布，使网络不会向一个特征过分偏移，也就减轻了过拟合。这点和dropout的防过拟合原理是类似的。（过拟合其实就是过分学习了某一些特征，过分学习了训练集在这一些特征上所体现出来的特性，而这些特性并不是网络表征的任务所需要的）

#### 4. dropout

work的原因：见op.md dropout

1. **减小神经元之间复杂的共适应关系**

   dropout使神经网络每次迭代一个神经元周围的神经元都在发生变化，这样**使每个神经元的学习不再依赖于有固定关系的其他神经元的共同作用，防止某些特征只在其他特征的影响下才有效的现象**，使得网络可以学习更鲁棒的特征。

2. **变相的模型集成**

   dropout掉不同的隐藏神经元就类似在训练不同的网络，整个dropout过程就相当于对很多个不同的神经网络取平均。

#### 5. model ensemble boosting bagging

模型集成

#### 6. simpler model

越复杂的网络，越容易出现过拟合。即奥卡姆剃刀原理。

所以适当减小网络的层数，减小网络复杂度，有利于抑制过拟合。

#### 7. 给模型权重加噪

### 从训练技巧上

#### 1. early stopping

每个epoch都计算验证集指标，当超过N个epoch验证集指标都没有提高时，可以停止训练了。

#### 2. 对抗训练

本质上也是数据增强，见地平线自己写的wiki

#### 3. shuffle数据集

参考：https://blog.csdn.net/qq_19672707/article/details/88864207

## 学习率的选择

